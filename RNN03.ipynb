{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Deep Learning II</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "Em poucas palavras, Word Embedding transforma texto em números. Essa transformação é necessária porque muitos algoritmos de aprendizado de máquina (incluindo redes profundas) exigem que sua entrada seja vetores de valores contínuos; Eles simplesmente não trabalharão em strings de texto simples.\n",
    "\n",
    "Assim, uma técnica de modelagem de linguagem natural, como o Word Embedding, é usada para mapear palavras ou frases de um vocabulário para um vetor correspondente de números reais. Além de ser acessível ao processamento por algoritmos de ML, essa representação vetorial possui duas propriedades importantes e vantajosas:\n",
    "\n",
    "* Dimensionality Reduction - é uma representação mais eficiente\n",
    "* Contextual Similarity - é uma representação mais expressiva\n",
    "\n",
    "Se você está familiarizado com a abordagem do Bag of Words (muito usado em Text Mining), você saberá que muitas vezes o resultado é um conjunto de vetores enormes, muito esparsos, onde a dimensionalidade dos vetores que representam cada documento é igual ao tamanho do vocabulário suportado. Word Embedding visa criar uma representação vetorial com um espaço dimensional muito menor.\n",
    "\n",
    "Word Embedding é usado para análise semântica, para extrair significado do texto a fim de permitir a compreensão da linguagem natural. Para que um modelo de linguagem seja capaz de prever o significado do texto, ele precisa estar ciente da similaridade contextual das palavras. Por exemplo, que tendemos a encontrar palavras de frutas (como maçã ou laranja) em frases onde elas são cultivadas, colhidas, comidas e bebidas, mas não esperamos encontrar esses mesmos conceitos tão perto de, digamos, a palavra avião.\n",
    "\n",
    "Os vetores criados pela Word Embedding preservam essas semelhanças, de modo que as palavras que ocorrem regularmente nas proximidades do texto também estarão próximas do espaço vetorial. \n",
    "\n",
    "Portanto, \"O que é Word Embedding?\" é um meio de construir uma representação vetorial de baixa dimensionalidade a partir do corpus de texto, que preserva a semelhança contextual das palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação de Sequências em Texto com LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classificação de sequências é um problema de modelagem preditiva em que você possui algumas sequências de dados ao longo do espaço ou do tempo e a tarefa é prever uma categoria para a sequência. O que torna esse problema difícil é que as sequências podem variar em comprimento, ser compostas por um vocabulário muito grande de símbolos de entrada e pode exigir que o modelo aprenda o contexto ou dependências de longo prazo entre símbolos na sequência de entrada. Vejamos como desenvolver um modelo de rede neural recorrente LSTM para problemas de classificação de sequência em Python usando o framework Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O problema que usaremos para demonstrar a aprendizagem de sequências é a classificação do sentimento dos reviews de filmes do IMDB. Podemos rapidamente desenvolver um pequeno modelo LSTM para o problema IMDB e obter uma boa precisão. \n",
    "\n",
    "http://www.imdb.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras as k\n",
    "k.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o conjunto de dados, mas mantém apenas as palavras n superiores, zerando o resto\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32])\n",
      " list([1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
      " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 2, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
      " ...\n",
      " list([1, 11, 6, 230, 245, 2, 9, 6, 1225, 446, 2, 45, 2174, 84, 2, 4007, 21, 4, 912, 84, 2, 325, 725, 134, 2, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 2, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 2, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 2, 2, 5, 4241, 18, 4, 2, 2, 250, 11, 1818, 2, 4, 4217, 2, 747, 1115, 372, 1890, 1006, 541, 2, 7, 4, 59, 2, 4, 3586, 2])\n",
      " list([1, 1446, 2, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 2, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23])\n",
      " list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 2, 9, 24, 6, 78, 1099, 17, 2345, 2, 21, 27, 2, 2, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 2, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 2, 8, 2197, 2, 2, 544, 5, 383, 1271, 848, 1468, 2, 497, 2, 8, 1597, 2, 2, 21, 60, 27, 239, 9, 43, 2, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Em seguida, precisamos truncar e ajustar as sequências de entrada para que elas tenham o mesmo comprimento \n",
    "# para modelagem. O modelo aprenderá que os valores zero não possuem informações, de modo que as sequências\n",
    "# não tem o mesmo comprimento em termos de conteúdo, mas os vetores de mesmo comprimento são necessários para \n",
    "# executar a computação no Keras.\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen = max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/layers/embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 87s 3ms/step - loss: 0.4826 - accuracy: 0.7604\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 86s 3ms/step - loss: 0.3034 - accuracy: 0.8788\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 88s 4ms/step - loss: 0.2556 - accuracy: 0.9015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc5c854bf10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cria o modelo\n",
    "# Agora podemos definir, compilar e ajustar nosso modelo LSTM. A primeira camada é a camada Embedding, \n",
    "# que usa 32 vetores para representar cada palavra. A próxima camada é a camada LSTM com 100\n",
    "# unidades de memória (neurônios inteligentes). Finalmente, como este é um problema de classificação, \n",
    "# usamos uma camada Densa com um único neurônio de saída e ativação sigmóide, que vai gerar previsões igual a 0 ou 1.\n",
    "# Por se tratar de um problema de classificação binária, usamos como a função de perda (crossentropy binário em Keras). O ADAM conhecido\n",
    "# O algoritmo de otimização ADAM é usado. O modelo é treinado por apenas 3 épocas pois ele rapidamente pode apresentar \n",
    "# overfitting. Um grande tamanho do lote de 64 avaliações é usado para espaçar as atualizações de peso.\n",
    "\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length = max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs = 3, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 84.69%\n"
     ]
    }
   ],
   "source": [
    "# Avaliação final do modelo\n",
    "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print(\"Acurácia: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode ver que este simples modelo LSTM com pouco ajuste atinge resultados próximos do estado da arte para o problema de classificação de texto do IMDB. Importante, este é um modelo que você pode usar para aplicar redes LSTM\n",
    "para seus próprios problemas de classificação de sequência. Agora, vejamos algumas extensões deste simples modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando Regularização com Dropout em LSTMs - Versão 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 85s 3ms/step - loss: 0.4858 - accuracy: 0.7552\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 85s 3ms/step - loss: 0.2949 - accuracy: 0.8818\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 88s 4ms/step - loss: 0.3382 - accuracy: 0.8652\n",
      "Acurácia: 84.28%\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Random seed \n",
    "numpy.random.seed(7)\n",
    "\n",
    "# Carrega o conjunto de dados, mas mantém apenas as palavras n superiores, zerando o resto\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = top_words)\n",
    "\n",
    "# Em seguida, precisamos truncar e ajustar as sequências de entrada para que elas tenham o mesmo comprimento \n",
    "# para modelagem. \n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen = max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = max_review_length)\n",
    "\n",
    "# Cria o modelo\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length = max_review_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs = 3, batch_size = 64)\n",
    "\n",
    "# Avaliação final do modelo\n",
    "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print(\"Acurácia: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando Regularização com Dropout em LSTMs - Versão 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 87s 3ms/step - loss: 0.5095 - accuracy: 0.7546\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 88s 4ms/step - loss: 0.3624 - accuracy: 0.8474\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 89s 4ms/step - loss: 0.3242 - accuracy: 0.8686\n",
      "Acurácia: 84.48%\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Random seed \n",
    "numpy.random.seed(7)\n",
    "\n",
    "# Carrega o conjunto de dados, mas mantém apenas as palavras n superiores, zerando o resto\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = top_words)\n",
    "\n",
    "# Em seguida, precisamos truncar e ajustar as sequências de entrada para que elas tenham o mesmo comprimento \n",
    "# para modelagem. \n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen = max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = max_review_length)\n",
    "\n",
    "# Cria o modelo\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length = max_review_length))\n",
    "model.add(LSTM(100, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs = 3, batch_size = 64)\n",
    "\n",
    "# Avaliação final do modelo\n",
    "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print(\"Acurácia: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs e CNNs Para Classificação de Sequências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As redes neurais convolucionais são excelentes para aprender a estrutura espacial em dados de entrada. No IMDB os dados de reviews têm uma estrutura espacial unidimensional na sequência de palavras nas revisões e uma CNN poderá escolher características invariantes para o sentimento bom e ruim. Estas características espaciais aprendidas podem então ser aprendidas como sequências por uma camada LSTM. Podemos facilmente adicionar uma CNN unidimensional e as camadas de Max Pooling após a camada Embedding que, em seguida, alimente os recursos consolidados para o LSTM. Podemos usar um pequeno conjunto de 32 recursos com um pequeno filtro de comprimento igual a 3. A camada de pooling pode usar o comprimento padrão de 2 para reduzir a metade o tamanho do mapa de recursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.4245 - accuracy: 0.7892\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2354 - accuracy: 0.9081\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.1856 - accuracy: 0.9296\n",
      "Acurácia: 88.37%\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Random seed \n",
    "numpy.random.seed(7)\n",
    "\n",
    "# Carrega o conjunto de dados, mas mantém apenas as palavras n superiores, zerando o resto\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = top_words)\n",
    "\n",
    "# Em seguida, precisamos truncar e ajustar as sequências de entrada para que elas tenham o mesmo comprimento \n",
    "# para modelagem. \n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "# Cria o modelo\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length = max_review_length))\n",
    "model.add(Convolution1D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs = 3, batch_size = 64)\n",
    "\n",
    "# Avaliação final do modelo\n",
    "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print(\"Acurácia: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
